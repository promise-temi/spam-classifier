{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Taille après SMOTE :\n",
      "X_train_resampled : (8132, 5013) y_train_resampled : 8132\n"
     ]
    }
   ],
   "source": [
    "# Importer les bibliothèques nécessaires\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "\n",
    "# Charger le dataset non prétraité (brut) et prétraité\n",
    "df_raw = pd.read_csv('../datasets/all_clean_.csv')  # Texte brut\n",
    "\n",
    "# Charger les données finales dans la variable df\n",
    "df = pd.read_csv('../datasets/preprocessed_df.csv')\n",
    "# Étape 2 : Nettoyer les données\n",
    "df = df.drop(columns=['is_eng'])  # Suppression des colonnes inutiles\n",
    "df = df.dropna(subset=['text'])  # Supprimer les lignes avec des valeurs manquantes\n",
    "\n",
    "# Étape 3 : Préparer les features (X) et les labels (y)\n",
    "text_data = df['text']  # Texte brut\n",
    "numeric_data = df[['len', 'len_no_spaces', 'n_words', 'n_special_chars', 'n_capital_letters', 'entities','w_free','w_click','w_claim', 'w_offer','w_win', 'w_prize', 'w_want']]  # Colonnes numériques\n",
    "y = df['label']  # Labels (spam/ham)\n",
    "\n",
    "# Étape 4 : Transformer le texte en vecteurs TF-IDF\n",
    "vectorizer = TfidfVectorizer(max_features=5000)\n",
    "X_text = vectorizer.fit_transform(text_data)\n",
    "\n",
    "# Étape 5 : Combiner les données TF-IDF avec les colonnes numériques\n",
    "X_combined = np.hstack((X_text.toarray(), numeric_data.to_numpy()))\n",
    "\n",
    "# Étape 6 : Séparer les données en entraînement/test avant d'appliquer SMOTE\n",
    "X_train, X_test, y_train, y_test, text_train, text_test = train_test_split(\n",
    "    X_combined, y, text_data, test_size=0.1, random_state=41                                                                                                                                                        , #random_state=41\n",
    ")\n",
    "\n",
    "# Étape 7 : Appliquer SMOTE uniquement sur les données d'entraînement\n",
    "smote = SMOTE(random_state=41)\n",
    "smote = SMOTE()\n",
    "X_train_resampled, y_train_resampled = smote.fit_resample(X_train, y_train)\n",
    "\n",
    "# Vérification des tailles après SMOTE\n",
    "print(\"Taille après SMOTE :\")\n",
    "print(\"X_train_resampled :\", X_train_resampled.shape, \"y_train_resampled :\", len(y_train_resampled))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Importer les bibliothèques nécessaires\n",
    "# import pandas as pd\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# from imblearn.over_sampling import SMOTE\n",
    "# import numpy as np\n",
    "# from gensim.models import Word2Vec\n",
    "# df_raw = pd.read_csv('../datasets/all_clean_.csv')\n",
    "# # Charger le dataset\n",
    "# df = pd.read_csv('../datasets/preprocessed_df.csv')\n",
    "\n",
    "# # Étape 3 : Préparer les features (X) et les labels (y)\n",
    "# text_data = df['text']  # Texte brut\n",
    "# numeric_data = df[['len', 'len_no_spaces', 'n_words', 'n_special_chars', 'n_capital_letters', 'emails', 'urls', 'phones']]  # Colonnes numériques\n",
    "# y = df['label']  # Labels (spam/ham)\n",
    "\n",
    "\n",
    "# # Étape 5 : Entraîner un modèle Word2Vec sur les textes\n",
    "# model_w2v = Word2Vec(sentences=text_data, vector_size=100, window=5, min_count=1, workers=4)\n",
    "\n",
    "# # Étape 6 : Fonction pour obtenir les embeddings Word2Vec pour un document\n",
    "# def get_word2vec_vector(doc, model_w2v):\n",
    "#     # Initialiser un vecteur vide pour le document\n",
    "#     doc_vector = np.zeros(model_w2v.vector_size)\n",
    "    \n",
    "#     # Ajouter les vecteurs des mots dans le document\n",
    "#     word_count = 0\n",
    "#     for word in doc:\n",
    "#         if word in model_w2v.wv:\n",
    "#             doc_vector += model_w2v.wv[word]\n",
    "#             word_count += 1\n",
    "    \n",
    "#     # Si aucun mot n'a été trouvé dans le modèle, retourner un vecteur zéro\n",
    "#     if word_count > 0:\n",
    "#         doc_vector /= word_count  # Moyenne des vecteurs\n",
    "#     return doc_vector\n",
    "\n",
    "# # Étape 7 : Appliquer la fonction de transformation à chaque document\n",
    "# X_text_w2v = np.array([get_word2vec_vector(doc, model_w2v) for doc in text_data])\n",
    "\n",
    "# # Étape 8 : Combiner les embeddings Word2Vec avec les colonnes numériques\n",
    "# X_combined = np.hstack((X_text_w2v, numeric_data.to_numpy()))\n",
    "\n",
    "# # Étape 9 : Séparer les données en entraînement/test avant d'appliquer SMOTE\n",
    "# X_train, X_test, y_train, y_test = train_test_split(\n",
    "#     X_combined, y, test_size=0.3, random_state=41\n",
    "# )\n",
    "\n",
    "# # Étape 10 : Appliquer SMOTE uniquement sur les données d'entraînement\n",
    "# smote = SMOTE(random_state=41)\n",
    "# X_train_resampled, y_train_resampled = smote.fit_resample(X_train, y_train)\n",
    "\n",
    "# # Vérification des tailles après SMOTE\n",
    "# print(\"Taille après SMOTE :\")\n",
    "# print(\"X_train_resampled :\", X_train_resampled.shape, \"y_train_resampled :\", len(y_train_resampled))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Précision Logistic Regression : 0.9749582637729549\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "# utiliser le modèle random forest pour prédire les données\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "#utiliser le modele de reseau de neurones pour prédire les données\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "#utliser le model bert pour prédire les données\n",
    "\n",
    "\n",
    "# Entraîner le modèle\n",
    "clf = RandomForestClassifier()\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Prédictions et évaluation\n",
    "y_pred = clf.predict(X_test)\n",
    "print(\"Précision Logistic Regression :\", accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matrice de confusion :\n",
      "[[450   0]\n",
      " [ 15 134]]\n",
      "Rapport de classification :\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      1.00      0.98       450\n",
      "           1       1.00      0.90      0.95       149\n",
      "\n",
      "    accuracy                           0.97       599\n",
      "   macro avg       0.98      0.95      0.97       599\n",
      "weighted avg       0.98      0.97      0.97       599\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "print(\"Matrice de confusion :\")\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "\n",
    "print(\"Rapport de classification :\")\n",
    "print(classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Faux positifs (hams classés comme spams) :\n"
     ]
    }
   ],
   "source": [
    "# Identifier les indices des faux positifs et négatifs\n",
    "faux_positifs = np.where((y_pred == 1) & (y_test == 0))[0]  # FP : Prédit spam mais vrai ham\n",
    "faux_negatifs = np.where((y_pred == 0) & (y_test == 1))[0]  # FN : Prédit ham mais vrai spam\n",
    "\n",
    "# Étape 5 : Visualiser les textes bruts pour les faux positifs et négatifs\n",
    "print(\"Faux positifs (hams classés comme spams) :\")\n",
    "for index in faux_positifs:\n",
    "    print(f\"Texte brut : {df_raw['text'].iloc[index]}\")  # Affiche le texte brut\n",
    "    print(f\"Prédiction : {y_pred[index]}, Vrai label : {y_test.iloc[index]}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Faux négatifs (spams classés comme hams) :\n",
      "Texte brut : n a d a [entity]\n",
      "Prédiction : 0, Vrai label : 1\n",
      "\n",
      "Texte brut : want to be dropped from our list do not reply to this email copy and paste this link into your browser bisops com rmm htm computer technologies 848 n rainbow blvd 316 las vegas nv[entity]\n",
      "Prédiction : 0, Vrai label : 1\n",
      "\n",
      "Texte brut : you gape for [explicit]like you had seen in [explicit] films now you have chance to do it become stronger show your volume worried it won t work don t be afraid delete me [entity]\n",
      "Prédiction : 0, Vrai label : 1\n",
      "\n",
      "Texte brut : alpha male plus the only multiple [explicit] supplement for men prevent premature ejaculaton become the ultimate [explicit] machine multiple [explicit]with no erection loss your easy to use solution is here [entity]\n",
      "Prédiction : 0, Vrai label : 1\n",
      "\n",
      "Texte brut : goodbye gilt chimiquepretend berra galacticablaze northeast whombloodstream hardhat entertainhaley periodic gyroscope arachnidsubstantive hookup formic pliocenechandelier charisma ginncommon\n",
      "Prédiction : 0, Vrai label : 1\n",
      "\n",
      "Texte brut : by using the internet obtaining a p rescription and m edication straight to your door has never been this easy v 1 s i t our s 1 t e and 0 r d e r h e r e\n",
      "Prédiction : 0, Vrai label : 1\n",
      "\n",
      "Texte brut : save up to 80 on popular meds great specials check it out k[entity]\n",
      "Prédiction : 0, Vrai label : 1\n",
      "\n",
      "Texte brut : if you re in need of a good rx site for online purchases we are your answer with tens of thousands of happy customers who saved huge you can t go wrong lots more info here above url is for more info if you are interested\n",
      "Prédiction : 0, Vrai label : 1\n",
      "\n",
      "Texte brut : you best friends and family deserve the best internet photo album software distribution a horse a horse my kingdom for a horse heroes are often the most ordinary of men\n",
      "Prédiction : 0, Vrai label : 1\n",
      "\n",
      "Texte brut : software should be easy to use seven days seven ways to save 10 off hard drivres there is no other rule a person s a person no matter how small\n",
      "Prédiction : 0, Vrai label : 1\n",
      "\n",
      "Texte brut : get the software you need now save up to 40 on popular software bundles last words are for people who haven t said anything in life human nature constitutes a part of the evidence in every case\n",
      "Prédiction : 0, Vrai label : 1\n",
      "\n",
      "Texte brut : [entity]\n",
      "Prédiction : 0, Vrai label : 1\n",
      "\n",
      "Texte brut : delivery status notification failure this is an automatically generated delivery status notification delivery to the following recipients failed [entity]\n",
      "Prédiction : 0, Vrai label : 1\n",
      "\n",
      "Texte brut : re [ 879 ] ladybug regain your confidence with the best generic [explicit] from a licensed manufacturer it is only one click away\n",
      "Prédiction : 0, Vrai label : 1\n",
      "\n",
      "Texte brut : [explicit] online now levitra is in the class of oral impotence medication like [explicit] whatever you are be a good one advice is a dangerous gift even from the wise to the wise men in however high a station ought to fear the humble\n",
      "Prédiction : 0, Vrai label : 1\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Faux négatifs (spams classés comme hams) :\")\n",
    "for index in faux_negatifs:\n",
    "    print(f\"Texte brut : {df_raw['text'].iloc[index]}\")  # Affiche le texte brut\n",
    "    print(f\"Prédiction : {y_pred[index]}, Vrai label : {y_test.iloc[index]}\")\n",
    "    print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
